Extract, organize, and integrate data from disparate sources.
Prepare data for analysis and reporting by transforming and cleansing it.
Design and manage data pipelines that encompass the journey of data from source to destination systems..
Setup and manage the infrastructure required for the ingestion, processing, and storage of data: Data platforms, data stores, distributed systems, data repositories.

# Technical Skills

Knowledge of working with operating systems such as UNIX, Linux, Windows, including commonly used administrative tools, system utilities, and commands.
Knowledge of infrastructure components, such as virtual machines, networking, and application service such as load balancing and application performance monitoring, cloud-based services.
Experience with databases and data warehouses: RDMBS such as IBM DB", MYSQL, Oracle DB, and PostgreSQL, NoSQL: Redis; MongoDB, Casandra and neo4j, and cloud DB services.
Data pipeline: Popular data pipeline solutions include Apache Beam, AirFlow, and DataFlow.
Experience of working with ELT tools such as IBM infoSphere Information Server, AWS Glue and improvado.
Proficiency in languages for querying, manipulating, and processing data: Query Languages for SQL and No-SQL queries, Programming Language: Python, R,Java, Shell and Scripting Languages.
Big data processing tools: Hadoop, Hive, and spark.
